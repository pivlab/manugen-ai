# copy this file to .env and fill in the values

# customize this value with your own domain name if you intend to run
# a production version of the app
DOMAIN_NAME="production_domain"

# specify the default model name to use
# you can use different models, such as:
# * ollama/qwen3:32b (local models)
# * openai/o4-mini
# * anthropic/claude-sonnet-4-20250514
MANUGENAI_MODEL_NAME="gemini-2.5-flash"

# (optional) specify the default model name for interpreting figures/images
#MANUGENAI_FIGURE_MODEL_NAME="openai/o4-mini"

# specify your OpenAI key if you're using this provider
#OPENAI_API_KEY=

# specify your Anthropic key if you are using this provider
#ANTHROPIC_API_KEY=

# specify your Google key if you are using this provider
GOOGLE_API_KEY=
GOOGLE_GENAI_USE_VERTEXAI=FALSE

# there are two options we support for generating text embeddings:
# - Gemini (default): uses GEMINI_EMBEDDING_MODEL_NAME via Google's GenAI API
# - FlagEmbedding's model: runs FLAGEMBEDDING_MODEL_OR_PATH locally

# if USE_GEMINI_EMBEDDINGS=1, the app will use Google's GenAI API for text embeddings
# otherwise, it will use FlagEmbedding's model for text embeddings
# note that using gemini embeddings requires a Google API key (i.e., GOOGLE_API_KEY)
USE_GEMINI_EMBEDDINGS=1

# gemini embedding options
# ---
# gemini embedding model name
# see https://ai.google.dev/gemini-api/docs/models#text-embedding for more options
# GEMINI_EMBEDDING_MODEL_NAME="gemini-embedding-exp-03-07"
GEMINI_EMBEDDING_MODEL_NAME="text-embedding-004"

# flagembedding options
# ---
# either the model name from huggingface or the path to the model
# in this case, we use the BAAI's BGE-M3 model
FLAGEMBEDDING_MODEL_OR_PATH="BAAI/bge-m3"
# where to store the downloaded model
FLAGEMBEDDING_CACHE_DIR="/opt/model_cache/"


# Ollama API host, running on the host machine
OLLAMA_API_BASE="http://localhost:11434"
